{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95288c2b-7871-4b4a-b0df-787e54942019",
   "metadata": {},
   "source": [
    "def thesis_expt_function(feat_sel_technique, model, dataset):\n",
    "    '''\n",
    "    takes in two function handles and returns a dataframe or series with one column per metric \n",
    "\n",
    "    Parameters: \n",
    "   ---------------\n",
    "    feat_sel_technique: xy_greater_xa, maximally_demographic_informative_subset, or MRMR\n",
    "    model : estimator object \n",
    "    dataset: csv file \n",
    "\n",
    "    Return \n",
    "-------- \n",
    "    resutl_df : one column per metric\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1e0ce3-175b-4378-ae40-3a29c2c03616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c9b0f3-272b-4919-8488-a9041bdb06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expttools import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d1d25-c891-42c4-935e-e54de49a6a80",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3c690a-1686-40b7-a37e-8da703587f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorical_feature_encoder(data,features):\n",
    "    '''\n",
    "    takes a data frame and returns numerical encoding for categorical features\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: csv file \n",
    "    features : list of categorical features\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe with encoded features, encoding for categorical features  \n",
    "    '''\n",
    "    enc = {}\n",
    "    \n",
    "    for f in features:\n",
    "        encoder = OrdinalEncoder()\n",
    "        data[f] = encoder.fit_transform(data[[f]]).astype(int)\n",
    "        enc[f] = encoder\n",
    "    return data, enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4b9799-84f3-417c-aad9-f4b06ddb5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_manipulation(features_encoded_data,data,col_name,priviliged_vals):\n",
    "    '''\n",
    "    takes encoded data frame and original data to substitute an original column from original df to \n",
    "    encoded df for further analysis\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    features_encoded_data: encoded dataset \n",
    "    data : original data\n",
    "    col_name : column for manipulation. eg: if comparison is done between black and white, this piece \n",
    "    of code will remove all other \n",
    "    races and return black as 0 white as 1 and \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe with encoded features containing manipulated column \n",
    "    '''\n",
    "    \n",
    "    decoded_col_name = f'{col_name}_decoded'\n",
    "    filtered_col_name = f'filtered_{col_name}'\n",
    "    \n",
    "    #priviliged_vals = ['Married-civ-spouse', 'Married-spouse-absent']\n",
    "    features_encoded_data[decoded_col_name] = data[col_name]\n",
    "    features_encoded_data[filtered_col_name] = features_encoded_data[decoded_col_name]\\\n",
    "    .isin(priviliged_vals).astype(int)\n",
    "\n",
    "    features_encoded_data = features_encoded_data.drop([decoded_col_name], axis = 1)\n",
    "\n",
    "    return features_encoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4603b8-1db9-409f-9c7a-e61a50b07fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def base_model(data,target,col_name):\n",
    "    '''\n",
    "    takes feature encoded data, splits for training and test set and returns the data frame with predictions.\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data : encoded data\n",
    "    target : proxy target\n",
    "    col_name : column for manipulation. \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : original_output -- predicted set\n",
    "    '''\n",
    "    encoded_df = data.copy()\n",
    "    x = encoded_df.drop([target], axis = 1)\n",
    "    y = encoded_df[target]\n",
    "    filtered_col_name = f'filtered_{col_name}'\n",
    "    #print(filtered_col_name)\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state = 0)\n",
    "    x_train[filtered_col_name] = x_train[filtered_col_name].apply(lambda x:1 if x>0 else 0)\n",
    "    x_test[filtered_col_name] = x_test[filtered_col_name].apply(lambda x:1 if x>0 else 0)\n",
    "    sc = StandardScaler()\n",
    "    x_train = pd.DataFrame(sc.fit_transform(x_train),columns = x_train.columns)\n",
    "    x_test = pd.DataFrame(sc.transform(x_test),columns = x_test.columns)\n",
    "    x_train[filtered_col_name] = x_train[filtered_col_name].apply(lambda x:1 if x>0 else 0)\n",
    "    x_test[filtered_col_name] = x_test[filtered_col_name].apply(lambda x:1 if x>0 else 0)\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    # We now need to add this array into x_test as a column for when we calculate the fairness metrics.\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    x_test['target_predicted'] = y_pred\n",
    "    original_output = x_test\n",
    "    original_output['actual'] = y_test.values\n",
    "    return original_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa353bf1-71e6-4964-8a6c-2db0fa4c8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairness_metrics_bc(original_output,col_name):\n",
    "    '''\n",
    "    takes prediction df and returns calculated fairness metrics \n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data : original_output with prediction column\n",
    "    col_name : column for manipulation. \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : fairness metrics\n",
    "    '''\n",
    "    filtered_col_name = f'filtered_{col_name}'\n",
    "\n",
    "    male_df = original_output[original_output[filtered_col_name] == 1]\n",
    "    num_of_priviliged = male_df.shape[0]\n",
    "    female_df = original_output[original_output[filtered_col_name] == 0]\n",
    "    num_of_unpriviliged = female_df.shape[0]\n",
    "\n",
    "    unpriviliged_outcomes = female_df[female_df['target_predicted'] == 1].shape[0]\n",
    "    unpriviliged = unpriviliged_outcomes/num_of_unpriviliged\n",
    "    unpriviliged\n",
    "\n",
    "    priviliged_outcomes = male_df[male_df['target_predicted'] == 1].shape[0]\n",
    "    priviliged = priviliged_outcomes/num_of_priviliged\n",
    "    priviliged\n",
    "\n",
    "    #Disparate impact\n",
    "    disparate_impact = unpriviliged / priviliged\n",
    "\n",
    "    #Statistical parity difference \n",
    "    statistical_parity_difference  = unpriviliged - priviliged\n",
    "    \n",
    "    #Equal opportunity difference\n",
    "    eod = original_output.copy()\n",
    "    eod = eod[eod['actual'] == 1] \n",
    "    eod ['true_positives'] = eod ['target_predicted'] == eod['actual']\n",
    "\n",
    "    eod_other = eod[eod[filtered_col_name]== 0]['true_positives'].mean()\n",
    "\n",
    "    eod_married_civ_absent = eod[eod[filtered_col_name] == 1]['true_positives'].mean()\n",
    "    equal_opportunity_difference  = eod_other - eod_married_civ_absent\n",
    "    \n",
    "    #accuracy\n",
    "    accuracy = (original_output['target_predicted']== original_output['actual']).mean()\n",
    "    \n",
    "    return ([disparate_impact,statistical_parity_difference,equal_opportunity_difference,accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd17f6-d4d4-4570-b078-2745a41ac21c",
   "metadata": {},
   "source": [
    "# Fair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c98dc0b-feb9-4426-9c06-0944fb4cbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 13:59:24.022277: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-13 13:59:24.402719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib\n",
      "2022-09-13 13:59:24.402742: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Requirements\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import AdultDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aif360.datasets import StructuredDataset, BinaryLabelDataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca67f2d-882e-4b76-a1e8-967d110ce837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_feature_encoder(data,features):\n",
    "    '''\n",
    "    takes a data frame and returns numerical encoding for categorical features\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: csv file \n",
    "    features : list of categorical features\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe with encoded features, encoding for categorical features  \n",
    "    '''\n",
    "    enc = {}\n",
    "    \n",
    "    for f in features:\n",
    "        encoder = OrdinalEncoder()\n",
    "        data[f] = encoder.fit_transform(data[[f]]).astype(int)\n",
    "        enc[f] = encoder\n",
    "    return data, enc\n",
    "\n",
    "#feature_encoded_data = categorical_feature_encoder(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941cbf98-2a40-406c-a245-6237131f0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_model(data, subset_cols, target, p_att):\n",
    "    \n",
    "    '''\n",
    "    takes encoded df and formats into a binary label df format to split and return test and train df.\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: original data \n",
    "    subset_cols : columns from dictionary generated from feature selection technique\n",
    "    target : proxy target\n",
    "    p_att: single protected attribute, key from dictionary generated from feature selection technique\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : dataset_orig_train, dataset_orig_test -- test and trained datasets \n",
    "    '''\n",
    "    \n",
    "    #subset_sex_cols = ['education-num', 'capital-gain', 'capital-loss', 'hours-per-week','education', 'native-country', 'workclass','sex','income-per-year']\n",
    "    encoded_df = data.copy()\n",
    "    structured_data = BinaryLabelDataset(favorable_label=1.0, unfavorable_label=0.0, df = encoded_df[subset_cols]\\\n",
    "                                         .dropna(), label_names = [target], protected_attribute_names = [p_att], \\\n",
    "                                         instance_weights_name=None, scores_names=[], unprivileged_protected_attributes\\\n",
    "                                         =[[0]], privileged_protected_attributes=[[1]], metadata=None)\n",
    "    dataset_orig = structured_data\n",
    "    privileged_groups = [{p_att: 1}] #male \n",
    "    unprivileged_groups = [{p_att: 0}] #female\n",
    "\n",
    "    dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "    scaler = StandardScaler()\n",
    "    dataset_orig_train.features = scaler.fit_transform(dataset_orig_train.features)\n",
    "    dataset_orig_test.features = scaler.transform(dataset_orig_test.features) \n",
    "    return dataset_orig_train,dataset_orig_test\n",
    "\n",
    "#dataset_orig_train,dataset_orig_test = fair_model(data, subset_cols, target, p_att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f730ea35-a5f7-439d-9695-2aecde60b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairness_metrics_fc(dataset_orig_train,dataset_orig_test,p_att,privileged_groups,unprivileged_groups,etas):\n",
    "    \n",
    "    '''\n",
    "    takes train and test dataset runs using PrejudiceRemover classifier and returns fairness metrics\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset_orig_train: training data set \n",
    "    dataset_orig_test: test dataset\n",
    "    p_att: single protected attribute, chooses particular key from dictionary generated from feature selection technique\n",
    "    privileged_groups : [1]\n",
    "    unprivileged_groups : [0]\n",
    "    etas : fairness penalty parameter\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : fairness metrics \n",
    "    '''\n",
    "    \n",
    "    #etas = [1]\n",
    "    outputs = []\n",
    "    #print(dataset_orig_train,dataset_orig_test,p_att,privileged_groups,unprivileged_groups,etas)\n",
    "    \n",
    "    for eta in etas:\n",
    "        debiased_model = PrejudiceRemover(eta=eta, sensitive_attr = p_att, class_attr=dataset_orig_train.label_names[0])        \n",
    "        model = debiased_model.fit(dataset_orig_train)\n",
    "        pred = model.predict(dataset_orig_test)\n",
    "\n",
    "        metric = ClassificationMetric(dataset_orig_test, pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        \n",
    "        outputs.append([eta,metric.disparate_impact(),metric.statistical_parity_difference(),metric.equal_opportunity_difference(),\\\n",
    "                        accuracy_score(pred.labels, dataset_orig_test.labels)])\n",
    "    return outputs\n",
    "\n",
    "#dataset_orig_train,dataset_orig_test, filtered_col,privileged_groups = [{filtered_col:1}] ,unprivileged_groups = [{filtered_col:0}],etas=etas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeac561-d71b-44be-a73c-60c8d85cfe57",
   "metadata": {},
   "source": [
    "### Feature selection technique - XY > XA : this technique generates dictionary with keys as protected att and values as features (all features where XY>XA for the particular protected att). We use the values (features) as subsets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40902067-79bc-423f-9019-2da06715ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8261c0-b9ab-4ea9-95d5-4e947df3ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features segregation \n",
    "features_num = ['hours-per-week', 'capital-gain', 'capital-loss','education-num' ] \n",
    "features_cat =  ['workclass','education', 'relationship', 'occupation', 'native-country'] \n",
    "protected_attributes = ['sex','race','age', 'marital-status']\n",
    "target = 'income-per-year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08edb81-5fcc-4777-aa18-c2fff41d96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categorical columns, OrdinalEncoder - each unique category value is assigned an integer value\n",
    "def categorical_feature_encoder(data,features):\n",
    "    '''\n",
    "    takes a data frame and returns numerical encoding for categorical features\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: csv file \n",
    "    features : list of categorical features\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe with encoded features, encoding for categorical features  \n",
    "    '''\n",
    "    enc = {}\n",
    "    \n",
    "    for f in features:\n",
    "        encoder = OrdinalEncoder()\n",
    "        data[f] = encoder.fit_transform(data[[f]]).astype(int)\n",
    "        enc[f] = encoder\n",
    "    return data, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824db0a3-da7e-47df-a404-fea9ebbcc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating mutual information score for each X (features) and Y (target)\n",
    "def calculate_miscore_xy(data, y_col,a_col):\n",
    "    '''\n",
    "    takes a data frame and returns a data frame with mutual information score between X(features) and Y(target)\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: csv file \n",
    "    y_col : target column\n",
    "    a_col : protected attributes \n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : returns mutual information score between X(features) and Y(target) in a dataframe \n",
    "    '''\n",
    "    mis_xy = []\n",
    "    x_cols = []\n",
    "    for x in data.columns:\n",
    "        if not (x in a_col): # skipping the demographic features \n",
    "            mis = mutual_info_score(data[x], data[y_col], contingency=None) # mis calculation\n",
    "            mis_xy.append(mis) \n",
    "            x_cols.append(x)\n",
    "\n",
    "\n",
    "    adult_dataFrame_feature_target = pd.DataFrame({'I(Xi,Y)': mis_xy, 'X': x_cols}) # creating pandas dataframe\n",
    "    adult_dataFrame_feature_target = adult_dataFrame_feature_target.loc[adult_dataFrame_feature_target['X'] != y_col]#loc : filtering dataframe based on index\n",
    "    adult_dataFrame_feature_target['Y'] = y_col # adding y column\n",
    "    return adult_dataFrame_feature_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2caf42-d25c-49e0-bc22-9d54d40f55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating mutual information score for each X (features) and A (demographic variables)\n",
    "\n",
    "def calculate_miscore_xa(data,protected_attributes):\n",
    "    '''\n",
    "    takes a data frame and returns a data frame with mutual information score between X(features) and A(protected attributes)\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    dataset: csv file \n",
    "    protected_attributes : protected attributes \n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : returns mutual information score between X(features) and A(protected attributes) in a dataframe \n",
    "    '''\n",
    "    \n",
    "    mis_xa = []\n",
    "    attribute_unfiltered = []\n",
    "    feature_unfiltered = []\n",
    "    for x in data.columns:\n",
    "        for a in protected_attributes:\n",
    "            if not (x in protected_attributes):\n",
    "                mis = mutual_info_score(data[a], data[x], contingency=None)\n",
    "                mis_xa.append(mis)\n",
    "                attribute_unfiltered.append(a)  \n",
    "                feature_unfiltered.append(x)\n",
    "\n",
    "    unfiltered_mis_adult_dataFrame = pd.DataFrame({'X':feature_unfiltered, 'A':attribute_unfiltered, 'I(Xi,A)': mis_xa})\n",
    "    return unfiltered_mis_adult_dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f44342-5172-4485-84da-6def60b748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging dataframes\n",
    "def generate_xy_greater_xa(data1,data2):\n",
    "    '''\n",
    "    takes two data frames (in our case two df generated for I(X,Y) and I(X,A) and returns a dictionary with keys as protected att and values as features \n",
    "    (features = all features where XY>XA for a particular protected att)\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data1: dataframe \n",
    "    data2: dataframe\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : dictionary with key as p attributes and values as features\n",
    "    '''\n",
    "    merged_xiY_xiA = pd.merge(data1,data2, on=['X'], how = 'inner') \n",
    "    merged_xiY_xiA\n",
    "    merged_xiY_xiA.to_csv('merged_xiY_xiA.csv')\n",
    "    #adding a new bool column, True if xi_A > xi_Y\n",
    "    merged_xiY_xiA['X_sub_A = True'] = merged_xiY_xiA['I(Xi,A)'] > merged_xiY_xiA['I(Xi,Y)']\n",
    "\n",
    "    # for each val of A pick list of features where XY>XA - X_sub_AÂ¯\n",
    "    dictonary_xy_greaterthan_xa = {} \n",
    "    for a_v, df_a in merged_xiY_xiA.groupby('A'):\n",
    "        dictonary_xy_greaterthan_xa[a_v] = df_a[df_a['X_sub_A = True']== False]['X'].values\n",
    "\n",
    "    return dictonary_xy_greaterthan_xa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab63ed-c498-4230-8a56-f0f4905d628b",
   "metadata": {},
   "source": [
    "# Building main driver function for base and fair classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "700f24f9-3db0-42a8-80cd-7ab07463cdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income-per-year</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>native-country</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>relationship</th>\n",
       "      <th>workclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Black</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>White</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>White</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Local-gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Black</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>White</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  education-num  sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0  25.0            7.0  1.0           0.0           0.0            40.0   \n",
       "1  38.0            9.0  1.0           0.0           0.0            50.0   \n",
       "2  28.0           12.0  1.0           0.0           0.0            40.0   \n",
       "3  44.0           10.0  1.0        7688.0           0.0            40.0   \n",
       "4  34.0            6.0  1.0           0.0           0.0            30.0   \n",
       "\n",
       "   income-per-year     education      marital-status native-country  \\\n",
       "0              0.0          11th       Never-married  United-States   \n",
       "1              0.0       HS-grad  Married-civ-spouse  United-States   \n",
       "2              1.0    Assoc-acdm  Married-civ-spouse  United-States   \n",
       "3              1.0  Some-college  Married-civ-spouse  United-States   \n",
       "4              0.0          10th       Never-married  United-States   \n",
       "\n",
       "          occupation   race   relationship  workclass  \n",
       "0  Machine-op-inspct  Black      Own-child    Private  \n",
       "1    Farming-fishing  White        Husband    Private  \n",
       "2    Protective-serv  White        Husband  Local-gov  \n",
       "3  Machine-op-inspct  Black        Husband    Private  \n",
       "4      Other-service  White  Not-in-family    Private  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading dataset\n",
    "adult_dataset = pd.read_csv('adult_dataset.csv')\n",
    "adult_dataset.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "adult_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f6eba-5bcc-47ec-8c43-bdcb31e63a60",
   "metadata": {},
   "source": [
    "### Model functions for selecting a particular model -- both can be called in main function by specifying model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3dd3ab-76e3-40d1-b45b-15b06d7d385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_classifier(features_encoded_data,target,p_att_col,model_cols,filtered_col, etas): # etas not in use but passing for consistency of format\n",
    "    '''\n",
    "    takes encoded df, runs logistic regression on the data to return a df consiting of fairness metrics and model type column.\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    features_encoded_data: encoded dataframe from categorical_feature_encoder function \n",
    "    target: proxy target\n",
    "    p_att_col : protected attributes in the data\n",
    "    model_cols : specific subset used from the data for analysis\n",
    "    filtered_col : used for filtering columns based on conditions eg. keeping black(0) and white(1) in race column and removing other races.\n",
    "    etas : fairness penalty parameter, not used in base classifier\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : fairness metric dataframe \n",
    "    '''\n",
    "    original_output = base_model(features_encoded_data[model_cols].copy(),target,p_att_col)\n",
    "    base_classifier_fairness_metrics = get_fairness_metrics_bc(original_output,p_att_col)\n",
    "    return pd.DataFrame([['base_model']+base_classifier_fairness_metrics],columns = ['model_type','disparate_impact','statistical_parity_difference',\\\n",
    "                                                                                     'equal_opportunity_difference','accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93283a75-8ba4-4257-a02f-3f5c2a743338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_classifier(features_encoded_data,target,p_att_col,model_cols, filtered_col, etas):\n",
    "    '''\n",
    "    takes encoded df, runs prejudice remover on the data to return a df consiting of fairness metrics and model type column.\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    features_encoded_data: encoded dataframe from categorical_feature_encoder function \n",
    "    target: proxy target\n",
    "    p_att_col : protected attributes in the data\n",
    "    model_cols : specific subset used from the data for analysis\n",
    "    filtered_col : used for filtering columns based on conditions eg. keeping black(0) and white(1) in race column and removing other races.\n",
    "    etas : fairness penalty parameter\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : fairness metric dataframe \n",
    "    '''\n",
    "    dataset_orig_train,dataset_orig_test = fair_model(features_encoded_data, model_cols, target, filtered_col)\n",
    "    fair_classifier_fairness_metrics = get_fairness_metrics_fc(dataset_orig_train,dataset_orig_test, filtered_col,privileged_groups = [{filtered_col:1}] ,\\\n",
    "                                                               unprivileged_groups = [{filtered_col:0}],etas=etas)\n",
    "    result = pd.DataFrame(fair_classifier_fairness_metrics,columns = ['eta','disparate_impact','statistical_parity_difference','equal_opportunity_difference',\\\n",
    "                                                                      'accuracy'])\n",
    "    result.insert(0,'model_type','fair_model')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d9db93-444d-4bef-9ea5-c03375d42bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data,features_encoded_data, p_att_col, dict_subsets_xy_greaterthan_xa,target, priviliged_vals, model_type,etas):\n",
    "    '''\n",
    "    build model takes encoded data and handles the data manipulation part prior to modeling phase (used for both fair and base classifier) and \n",
    "    finally runs the model and returns output dataframe with fairness metrics\n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data : original dataframe\n",
    "    features_encoded_data: encoded dataframe from categorical_feature_encoder function \n",
    "    target: proxy target\n",
    "    p_att_col : protected attributes in the data\n",
    "    dict_subsets_xy_greaterthan_xa : generated from generate_xy_greater_xa function\n",
    "    target : proxy target\n",
    "    priviliged_vals : priviliged value in a particular protected att, which should be considered as priviliged group. eg. Whites in races can be \n",
    "    considered as priviliged\n",
    "    model_type : fair or base models \n",
    "    etas : fairness penalty parameter for fair model - prejudice remover\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe \n",
    "    '''\n",
    "    filtered_col = f'filtered_{p_att_col}'\n",
    "    model_cols = [filtered_col] + list(dict_subsets_xy_greaterthan_xa[p_att_col]) + [target]\n",
    "    features_encoded_data = df_manipulation(features_encoded_data,data,p_att_col,priviliged_vals)\n",
    "    output = model_type(features_encoded_data,target,p_att_col,model_cols,filtered_col, etas)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab246d-1553-490b-8226-4cbfcff821ac",
   "metadata": {},
   "source": [
    "### feature selection technique -- called in main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8584e44e-9a96-4c21-88ed-8a1ebb317914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_greater_xa(data,features_cat,protected_attributes, target):\n",
    "    '''\n",
    "    takes data and returns encoded df and a dictionary with keys as protected att and values as features \n",
    "    (features = all features where XY>XA for a particular protected att) \n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data : original dataframe\n",
    "    features_cat: list of categorical features in data \n",
    "    protected_attributes : list of protected attributes in data\n",
    "    target: proxy target\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : encoded dataframe and a dictionary\n",
    "    '''\n",
    "    features_encoded_data,enc = categorical_feature_encoder(data.copy(),features_cat + protected_attributes)\n",
    "\n",
    "    mi_Xi_Y = calculate_miscore_xy(features_encoded_data, target, protected_attributes)\n",
    "    mi_Xi_A = calculate_miscore_xa(features_encoded_data,protected_attributes)\n",
    "    dict_subsets_xy_greaterthan_xa = generate_xy_greater_xa(mi_Xi_Y,mi_Xi_A) \n",
    "    return features_encoded_data, dict_subsets_xy_greaterthan_xa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f996608-303a-489b-9d89-19c5f2a2ec5d",
   "metadata": {},
   "source": [
    "### main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce59a3ce-e814-43af-b5a8-d31e7227d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_exp_bf_func(data,features_cat,protected_attributes, target, p_att_col, priviliged_vals, etas, technique = xy_greater_xa, \\\n",
    "                     model = fair_classifier):\n",
    "   \n",
    "    '''\n",
    "    takes data and other arguments to return fairness metrics for a particular model and feature selection technique \n",
    "    \n",
    "    Parameters: \n",
    "    ----------- \n",
    "    data : original dataframe\n",
    "    features_cat: list of categorical features in data \n",
    "    protected_attributes : list of protected attributes in data\n",
    "    target: proxy target\n",
    "    p_att_col : single protected attribute we want an analysis for\n",
    "    priviliged_vals : privileged values for feature mentioned in p_att_col\n",
    "    etas : fairness penalty parameter\n",
    "    technique : feature selection technique used for subset generation\n",
    "    model : estimator object, fair or base classifier\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    returns : dataframe with model type column and fairness metrics\n",
    "    '''\n",
    "    features_encoded_data, dict_subsets_xy_greaterthan_xa = technique(data,features_cat,protected_attributes, target)\n",
    "    '''\n",
    "    notes:\n",
    "    - split the data here : couldn't do it here as both models have different ways for split (fair classifier is first conversted to binary label dataset and \n",
    "    then it's split into train and test df, it differes from the process of logistic regression)\n",
    "    - instantiate the model object : \n",
    "    - fit : even fit differs so I did this part individually for both\n",
    "    '''\n",
    "    return build_model(data,features_encoded_data, p_att_col, dict_subsets_xy_greaterthan_xa,target, priviliged_vals, model, etas)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e518bea-9344-4a8b-8723-749a30e611db",
   "metadata": {},
   "source": [
    "### Arguments to be passed in main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c05abf-e7ea-4030-885f-a611d4cecb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>eta</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fair_model</td>\n",
       "      <td>1</td>\n",
       "      <td>0.146126</td>\n",
       "      <td>-0.148876</td>\n",
       "      <td>-0.222169</td>\n",
       "      <td>0.809538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type  eta  disparate_impact  statistical_parity_difference  \\\n",
       "0  fair_model    1          0.146126                      -0.148876   \n",
       "\n",
       "   equal_opportunity_difference  accuracy  \n",
       "0                     -0.222169  0.809538  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_num = ['hours-per-week', 'capital-gain', 'capital-loss','education-num' ] # all the numerical fearures\n",
    "features_cat =  ['workclass','education', 'relationship', 'occupation', 'native-country'] # categorical features\n",
    "protected_attributes = ['sex','race','age', 'marital-status'] # protected attributes\n",
    "target = 'income-per-year' # proxy target\n",
    "p_att_col = 'sex' # calculating on individual p_att 'sex'\n",
    "priviliged_vals = [1] # [priviliged_vals = [white] for white vs all ; priviliged_vals = [white,asian] if both are considered privileged]\n",
    "model = fair_classifier # it can be fair or base model\n",
    "technique = xy_greater_xa # geature selection technique\n",
    "etas = [1]\n",
    "# import feature selection technique to main func - done\n",
    "# calling main function with model type, technique and other args -  done\n",
    "\n",
    "main_exp_bf_func(adult_dataset,features_cat,protected_attributes, target, p_att_col, priviliged_vals , etas, technique, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "101d7121-7dd8-4eef-af8a-eee1aa7453e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_sex\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_model</td>\n",
       "      <td>0.179197</td>\n",
       "      <td>-0.153427</td>\n",
       "      <td>-0.223399</td>\n",
       "      <td>0.808845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type  disparate_impact  statistical_parity_difference  \\\n",
       "0  base_model          0.179197                      -0.153427   \n",
       "\n",
       "   equal_opportunity_difference  accuracy  \n",
       "0                     -0.223399  0.808845  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_num = ['hours-per-week', 'capital-gain', 'capital-loss','education-num' ] \n",
    "features_cat =  ['workclass','education', 'relationship', 'occupation', 'native-country'] \n",
    "protected_attributes = ['sex','race','age', 'marital-status']\n",
    "target = 'income-per-year'\n",
    "p_att_col = 'sex'\n",
    "priviliged_vals = [1] # [white vs all] [white,asian]\n",
    "model = base_classifier\n",
    "technique = xy_greater_xa\n",
    "etas = [1]\n",
    "# import feature selection technique to main func\n",
    "# calling main function with model type, technique and other args\n",
    "\n",
    "main_exp_bf_func(adult_dataset,features_cat,protected_attributes, target, p_att_col, priviliged_vals , etas, technique, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c76125-c1e2-4481-a18a-c8351c312091",
   "metadata": {},
   "source": [
    "## Exp tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8b445-fd2a-4462-ac88-08f17ec88531",
   "metadata": {},
   "source": [
    "I tried doing this part but files being generated are marked as failed. Can you please check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "630f8658-5ce3-4485-a3a4-fc6e474c8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debiased_model = PrejudiceRemover(eta=eta, sensitive_attr = p_att, class_attr=dataset_orig_train.label_names[0])\n",
    "features_num = ['hours-per-week', 'capital-gain', 'capital-loss','education-num' ] \n",
    "features_cat =  ['workclass','education', 'relationship', 'occupation', 'native-country'] \n",
    "protected_attributes = ['sex','race','age', 'marital-status']\n",
    "target = 'income-per-year'\n",
    "p_att_col = 'sex'\n",
    "priviliged_vals = [1] # [white vs all] [white,asian]\n",
    "model = base_classifier\n",
    "technique = xy_greater_xa\n",
    "etas = [1]\n",
    "\n",
    "\n",
    "thesis_param_grid = {'data':[adult_dataset],'features_cat': [features_cat], 'protected_attributes':[protected_attributes],\\\n",
    "                     'target': [target], 'p_att_col': [p_att_col],'priviliged_vals':[priviliged_vals] , 'etas': [etas], \\\n",
    "                     'technique': [technique], 'model': [model]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1319ff9f-31e9-4e3f-b7f4-dae2855d57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_expt = Experiment(main_exp_bf_func,thesis_param_grid)\n",
    "batchname, successes ,fails = my_expt.run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9c11af1-d653-4df3-b129-8f4a5a1f464d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/pi_brownsarahm_uri_edu/surbhi_uri/var_selection'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "388f5f59-4b4a-4878-9d4f-14736dd9be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24ccf6-dbb7-4b7c-b9da-4f36c4521653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
